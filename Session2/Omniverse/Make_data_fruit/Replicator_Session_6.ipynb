{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Model Traning\n",
        "\n",
        "> Basically It's highly recommended that following DLI turorial first\n",
        "> During your own dataset training, modify below parts for your perference."
      ],
      "metadata": {
        "id": "6yvlfEL_bsI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Traning epochs and semantic class numbers"
      ],
      "metadata": {
        "id": "Pyky1vApeHOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyJZHfUMbeuM"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "num_classes = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Directory Paths"
      ],
      "metadata": {
        "id": "DPHq3DhqeNCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/home/kimsooyoung/Documents/grocery_data_2024-05-23_16:43:00\"\n",
        "output_file = \"/home/kimsooyoung/Documents/model.pth\""
      ],
      "metadata": {
        "id": "IJK_OU5keQLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Static Labels"
      ],
      "metadata": {
        "id": "Ow-n9X-heRXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels for the dataset\n",
        "static_labels = {\n",
        "    'apple' : 0,\n",
        "    'avocado' : 1,\n",
        "    'kiwi' : 2,\n",
        "    'lime' : 3,\n",
        "    'lychee' : 4,\n",
        "    'pomegranate' : 5,\n",
        "    'onion' : 6,\n",
        "    'strawberry' : 7,\n",
        "    'lemon' : 8,\n",
        "    'orange' : 9\n",
        "}"
      ],
      "metadata": {
        "id": "SDMMGxM3eU5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Batch Size"
      ],
      "metadata": {
        "id": "Om7dPhhteWwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create our dataset by using our custom GroceryDataset class\n",
        "# This is then passed into our DataLoader.\n",
        "dataset = FruitDataset(data_dir, get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16, # 8 or 4\n",
        "    shuffle=True,\n",
        "    collate_fn= collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "SdkvXTzjeblB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Model Export\n",
        "\n",
        "> Basically It's highly recommended that following DLI turorial first\n",
        "> During your own dataset training, modify below parts for your perference.\n",
        "\n",
        "if your camera resolution was (1280, 720)\n",
        "\n",
        "```python\n",
        "self.rp = rep.create.render_product(self.cam, resolution=(1280, 720))\n",
        "```\n",
        "\n",
        "you must modify `dummy_input` as belows\n",
        "\n",
        "\n",
        "```python\n",
        "# Export Model\n",
        "width = 1280\n",
        "height = 720\n",
        "dummy_input = torch.rand(1, 3, height, width).cuda()\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "lnBq5XXweZAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Model Inference\n",
        "\n",
        "> This example will use [NVIDIA's Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) to serve the model exported in the previous section.\n",
        "\n",
        "- Select one pic from dataset or make new own from replicator\n",
        "- Run Inference with trained models and save the output\n",
        "- Vizualize output    \n",
        "\n",
        "![](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F5fbee23e-d9ec-4824-b2be-c8716a8602cd%2Ffb1d2d9b-ec88-44f0-9674-ce696073a4ad%2FUntitled.png?table=block&id=11da0ea2-0123-4a1c-a549-4987a6c471d9&spaceId=5fbee23e-d9ec-4824-b2be-c8716a8602cd&width=2000&userId=b9f59011-0253-43ef-946a-512501504ba8&cache=v2)\n"
      ],
      "metadata": {
        "id": "Gmh-YO_6fsAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NGC Triton Docker example\n",
        "\n",
        "- Assume that you made and saved model into `/home/<my-pc-name>/Documents/model_repository/`\n",
        "- You can pull and run docker image like this\n",
        "\n",
        "\n",
        "```\n",
        "docker run --gpus=1 --rm -p8000:8000 -p8003:8001 -p8004:8002 -v /home/<my-pc-name>/Documents/model_repository/:/models nvcr.io/nvidia/tritonserver:23.01-py3 tritonserver --model-repository=/models --model-control-mode=poll\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "n850DBXGe0ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> check this terminal log for successful import\n",
        "\n",
        "![](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F5fbee23e-d9ec-4824-b2be-c8716a8602cd%2Fcf09d2ed-5519-40c6-b816-af4e984c4b0d%2FUntitled.png?table=block&id=9cd9b07f-374e-4f51-9b2d-684e7c1abd17&spaceId=5fbee23e-d9ec-4824-b2be-c8716a8602cd&width=2000&userId=b9f59011-0253-43ef-946a-512501504ba8&cache=v2)"
      ],
      "metadata": {
        "id": "XLhIUejAiZtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructor Demonstration - Conveyor Belt in Grocery Factory\n",
        "\n",
        "![](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F5fbee23e-d9ec-4824-b2be-c8716a8602cd%2Ff0eebfe2-24b2-4be0-b64b-b8fe754c3047%2FScreenshot_from_2024-07-31_00-11-59.png?table=block&id=f7274350-b8de-4743-8dda-bcf68a31f410&spaceId=5fbee23e-d9ec-4824-b2be-c8716a8602cd&width=2000&userId=b9f59011-0253-43ef-946a-512501504ba8&cache=v2)"
      ],
      "metadata": {
        "id": "iup5QNk9koBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> If you want to add annotation labels like instructor...\n",
        "\n",
        "![](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F5fbee23e-d9ec-4824-b2be-c8716a8602cd%2F68ee5a1f-4bb7-4fce-b86a-3f3652410e3a%2Fannotated_img.png?table=block&id=6b7a9974-b76e-4ed8-bee5-7bf0e22a9a9a&spaceId=5fbee23e-d9ec-4824-b2be-c8716a8602cd&width=2000&userId=b9f59011-0253-43ef-946a-512501504ba8&cache=v2)"
      ],
      "metadata": {
        "id": "UotkK1sfkCmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define `props_dict` like below\n",
        "\n",
        "```python\n",
        "props_dict = {\n",
        "    0: 'klt_bin',\n",
        "    1: 'tomato_soup',\n",
        "    2: 'tuna',\n",
        "    3: 'spam',\n",
        "    4: 'jelly',\n",
        "    5: 'cleanser',\n",
        "}\n",
        "```\n",
        "\n",
        "2. Modify viz function like below\n",
        "\n",
        "```python\n",
        "if boxes.size > 0:  # ensure something is found\n",
        "    for box, lab, scr in zip(boxes, labels, scores):\n",
        "        if scr > 0.4:\n",
        "            box_top_left = int(box[0]), int(box[1])\n",
        "            box_bottom_right = int(box[2]), int(box[3])\n",
        "            text_origin = int(box[0]), int(box[3])\n",
        "\n",
        "            border_color = list(np.random.random(size=3) * 256)\n",
        "            text_color = (255, 255, 255)\n",
        "\n",
        "            font_scale = 0.9\n",
        "            thickness = 1\n",
        "\n",
        "            # bounding box2\n",
        "            img = cv2.rectangle(\n",
        "                annotated_image,\n",
        "                box_top_left,\n",
        "                box_bottom_right,\n",
        "                border_color,\n",
        "                thickness=5,\n",
        "                lineType=cv2.LINE_8\n",
        "            )\n",
        "\n",
        "            print(f\"index: {lab}, label: {props_dict[lab]}, score: {scr:.2f}\")\n",
        "\n",
        "            # For the text background\n",
        "            # Finds space required by the text so that we can put a background with that amount of width.\n",
        "            (w, h), _ = cv2.getTextSize(\n",
        "                props_dict[lab], cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.6, 1\n",
        "            )\n",
        "\n",
        "            # Prints the text.    \n",
        "            img = cv2.rectangle(\n",
        "                img, (box_top_left[0], box_top_left[1] - 20),\n",
        "                (box_top_left[0] + w, box_top_left[1]),\n",
        "                border_color, -1\n",
        "            )\n",
        "            img = cv2.putText(\n",
        "                img, props_dict[lab], box_top_left,\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
        "                text_color, 1\n",
        "            )\n",
        "    \n",
        "```"
      ],
      "metadata": {
        "id": "p9S3qm3BiHuy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPDBp2CmlE23"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}